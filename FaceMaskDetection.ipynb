{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32 \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.149066797642437, 35.0049115913556)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "for annotation_file in sorted(os.listdir('./annotations')):\n",
    "    annotation_path = os.path.join('./annotations', annotation_file)\n",
    "\n",
    "    # https://www.kaggle.com/bitthal/understanding-input-data-and-loading-with-pytorch\n",
    "    tree = tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    for obj in objects:\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        widths.append(xmax - xmin)\n",
    "        heights.append(ymax - ymin)\n",
    "        \n",
    "np.mean(widths), np.mean(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 3257), (3257, 3664), (3664, 4072))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_images = 4072\n",
    "train_indices = (0, int(0.80 * total_num_images))\n",
    "val_indices = (int(0.80 * total_num_images), int(0.9 * total_num_images))\n",
    "test_indices =  (int(0.9 * total_num_images), total_num_images)\n",
    "train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMaskDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, indices):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dict = {'with_mask': 0, 'without_mask': 1, 'mask_weared_incorrect': 2}\n",
    "        self.faces = []\n",
    "        \n",
    "        for annotation_file in sorted(os.listdir(annotations_dir)):\n",
    "            annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "\n",
    "            # https://www.kaggle.com/bitthal/understanding-input-data-and-loading-with-pytorch\n",
    "            tree = tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            filename = root.find('filename').text\n",
    "            objects = root.findall('object')\n",
    "            for obj in objects:\n",
    "                label = self.label_dict[obj.find('name').text]\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "                bbox = (xmin, ymin, xmax, ymax)\n",
    "                self.faces.append((filename, bbox, label))\n",
    "        \n",
    "        self.faces = self.faces[indices[0]:indices[1]]\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.faces)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.img_dir, self.faces[idx][0])\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        cropped_img = img.crop(self.faces[idx][1])\n",
    "        # Normalize stats from ImageNet\n",
    "        compose = T.Compose([\n",
    "            T.Resize((35, 35)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        return compose(cropped_img), self.faces[idx][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 35, 35])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_face_mask_dataset = FaceMaskDataset(img_dir='./images', annotations_dir='./annotations', indices=train_indices)\n",
    "train_face_mask_dataloader = DataLoader(train_face_mask_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "valid_face_mask_dataset = FaceMaskDataset(img_dir='./images', annotations_dir='./annotations', indices=val_indices)\n",
    "valid_face_mask_dataloader = DataLoader(valid_face_mask_dataset, batch_size=128)\n",
    "\n",
    "test_face_mask_dataset = FaceMaskDataset(img_dir='./images', annotations_dir='./annotations', indices=test_indices)\n",
    "test_face_mask_dataloader = DataLoader(test_face_mask_dataset, batch_size=128)\n",
    "\n",
    "# Boiler plate code to display some of the images\n",
    "for img, label in DataLoader(FaceMaskDataset(img_dir='./images', annotations_dir='./annotations', indices=test_indices), batch_size=1):\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.numpy()[0].transpose(1, 2, 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_face_mask_dataloader), len(valid_face_mask_dataloader), len(test_face_mask_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced Nikhil Pathak's ECE 176 Final Project on Image Colorization: https://github.com/nlpathak/ImageColorization\n",
    "def evaluate(model, loss_func, current_best):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in valid_face_mask_dataloader:\n",
    "            imgs = imgs.to(device=device, dtype=dtype) \n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "            scores = model(imgs)\n",
    "            \n",
    "            losses.append(loss_func(scores, labels).item())\n",
    "            accuracies.append(torch.sum(torch.argmax(scores, axis=1) == labels).item() / labels.shape[0])\n",
    "    \n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    \n",
    "    if mean_loss < current_best[\"loss\"]:\n",
    "        current_best[\"loss\"] = mean_loss\n",
    "        current_best[\"acc\"] = mean_acc\n",
    "        current_best[\"model\"] = copy.deepcopy(model)\n",
    "        \n",
    "    return mean_loss, mean_acc, current_best\n",
    "        \n",
    "def train(model, optimizer, loss_func, epochs=1):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    current_best = {\n",
    "        \"loss\": float('inf'),\n",
    "        \"acc\": 0, \n",
    "        \"model\": model\n",
    "    } \n",
    "    \n",
    "    model = model.to(device=device) \n",
    "    for e in trange(epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        for imgs, labels in train_face_mask_dataloader:\n",
    "            model.train()  \n",
    "            imgs = imgs.to(device=device, dtype=dtype)  \n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(imgs)\n",
    "            loss = loss_func(scores, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(torch.sum(torch.argmax(scores, axis=1) == labels).item() / labels.shape[0])\n",
    "\n",
    "        val_loss, val_acc, current_best = evaluate(model, loss_func, current_best)\n",
    "        \n",
    "        train_losses.append(np.mean(epoch_loss))\n",
    "        train_accs.append(np.mean(epoch_acc))\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {e}, train loss = {train_losses[-1]:.6f}, train acc = {train_accs[-1]:.6f}, \\\n",
    "                val loss = {val_losses[-1]:.6f}, val acc = {val_accs[-1]:.6f}')\n",
    "        \n",
    "    return train_losses, train_accs, val_losses, val_accs, current_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:56<17:55, 56.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss = 0.296228, train acc = 0.908359,                 val loss = 0.276405, val acc = 0.955927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [01:48<16:33, 55.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss = 0.108440, train acc = 0.964469,                 val loss = 0.140588, val acc = 0.948115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [02:40<15:24, 54.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss = 0.064748, train acc = 0.979794,                 val loss = 0.152692, val acc = 0.948115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5cc2c5e92ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-521eb1a35836>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_func, epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_face_mask_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5e44f819442e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print(image_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(img.size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "train_losses, train_accs, val_losses, val_accs, best = train(model, optimizer, nn.CrossEntropyLoss(), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
